{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DoubleDQN_gfootball.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhh8PBYRHpmY5wtyMGvxWV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lee-1997/fuzzy-tribble/blob/master/DoubleDQN_gfootball.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lVKJ16SgXcb",
        "colab_type": "text"
      },
      "source": [
        "## Double DQN算法应用于academy_empty_goal环境："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jE06NvKhn3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install libsdl2-gfx-dev libsdl2-ttf-dev\n",
        "\n",
        "# Make sure that the Branch in git clone and in wget call matches !!\n",
        "!git clone -b v2.0.6 https://github.com/google-research/football.git\n",
        "!mkdir -p football/third_party/gfootball_engine/lib\n",
        "\n",
        "!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.0.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n",
        "!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwGMewjtrXDn",
        "colab_type": "code",
        "outputId": "40cd511a-49e1-4635-a2e3-b1136e432086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import gfootball.env as football_env\n",
        "import collections\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "hyper_episodes = 1000\n",
        "hyper_gamma = 0.95\n",
        "hyper_gradient_step = 1e-3\n",
        "hyper_soft_update = 0.999\n",
        "hyper_batch_size = 50\n",
        "hyper_replay_size = 2000\n",
        "hyper_replay_start = 200\n",
        "hyper_epsilon_start = 1.0\n",
        "hyper_epsilon_decay = 0.99\n",
        "hyper_epsilon_final = 0.\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "class NeuralNetworks(nn.Module):\n",
        "  def __init__(self, input_size, output_size, embedding_size=100, hidden_size=20):\n",
        "    super(NeuralNetworks, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, embedding_size)\n",
        "    self.bn1 = nn.BatchNorm1d(embedding_size)\n",
        "    self.fc2 = nn.Linear(embedding_size, hidden_size)\n",
        "    self.bn2 = nn.BatchNorm1d(hidden_size)\n",
        "    self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.bn3 = nn.BatchNorm1d(hidden_size)\n",
        "    self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.bn4 = nn.BatchNorm1d(hidden_size)\n",
        "    self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.bn5 = nn.BatchNorm1d(hidden_size)\n",
        "    self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.bn6 = nn.BatchNorm1d(hidden_size)\n",
        "    self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.bn7 = nn.BatchNorm1d(hidden_size)\n",
        "    self.fc8 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.bn8 = nn.BatchNorm1d(hidden_size)\n",
        "    self.fc9 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.bn9 = nn.BatchNorm1d(hidden_size)\n",
        "    self.fc10 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.bn10 = nn.BatchNorm1d(hidden_size)\n",
        "    self.fc11 = nn.Linear(hidden_size, output_size)    \n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.bn1(self.fc1(x)))\n",
        "    x = F.relu(self.bn2(self.fc2(x)))\n",
        "    x = F.relu(x + self.bn4(self.fc4(F.relu(self.bn3(self.fc3(x))))))\n",
        "    x = F.relu(x + self.bn6(self.fc6(F.relu(self.bn5(self.fc5(x))))))\n",
        "    x = F.relu(x + self.bn8(self.fc8(F.relu(self.bn7(self.fc7(x))))))\n",
        "    x = F.relu(x + self.bn10(self.fc10(F.relu(self.bn9(self.fc9(x))))))\n",
        "    x = self.fc11(x)\n",
        "    return x\n",
        "\n",
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceReplay:\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "    states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "    return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), np.array(dones, dtype=np.uint8), np.array(next_states)\n",
        "\n",
        "\n",
        "class DdqnAgent:\n",
        "  def __init__(self, env, exp_buffer):\n",
        "    self.env = env\n",
        "    self.exp_buffer = exp_buffer\n",
        "    self._reset()\n",
        "\n",
        "  def _reset(self):\n",
        "    self.state = env.reset()\n",
        "    self.total_reward = 0.0\n",
        "\n",
        "  def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "    done_reward = None\n",
        "    if np.random.random() < epsilon:\n",
        "      action = env.action_space.sample()\n",
        "    else:\n",
        "      net.eval()\n",
        "      state_a = np.array([self.state], copy=False)\n",
        "      state_re = torch.tensor(state_a)\n",
        "      qvalus_re = net(state_re)\n",
        "      _, act_re = torch.max(qvalus_re, dim=1)\n",
        "      action = int(act_re.item())\n",
        "  \n",
        "    new_state, reward, is_done, _ = self.env.step(action)\n",
        "    self.total_reward += reward\n",
        "    exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "    self.exp_buffer.append(exp)\n",
        "    self.state = new_state\n",
        "    if is_done:\n",
        "      done_reward = self.total_reward\n",
        "      self._reset()\n",
        "    return done_reward\n",
        "\n",
        "\n",
        "def loss_calculation(batch, net, target_net, device=\"cpu\"):\n",
        "  states, actions, rewards, dones, next_states = batch\n",
        "  states_re = torch.tensor(states)\n",
        "  next_states_re = torch.tensor(next_states)\n",
        "  actions_re = torch.tensor(actions)\n",
        "  rewards_re = torch.tensor(rewards)\n",
        "  done_mask = torch.ByteTensor(dones)\n",
        "  state_action_values = net(states_re).gather(1, actions_re.unsqueeze(-1)).squeeze(-1)\n",
        "  next_state_values = net(next_states_re)\n",
        "  _, next_action_re = torch.max(next_state_values, dim=1)\n",
        "  next_state_action_values = target_net(next_states_re).gather(1, next_action_re.unsqueeze(-1)).squeeze(-1)\n",
        "  next_state_action_values[done_mask] = 0.0\n",
        "  next_state_action_values = next_state_action_values.detach()\n",
        "  expected_state_action_values = next_state_action_values * hyper_gamma + rewards_re\n",
        "  return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  env = football_env.create_environment(env_name=\"academy_empty_goal\", representation='simple115', number_of_left_players_agent_controls=1, stacked=False, logdir='/tmp/football', write_goal_dumps=False, write_full_episode_dumps=False, render=False)\n",
        "  net = NeuralNetworks(env.observation_space.shape[0], env.action_space.n)\n",
        "  target_net = NeuralNetworks(env.observation_space.shape[0], env.action_space.n)\n",
        "  target_net.load_state_dict(net.state_dict())\n",
        "  buffer = ExperienceReplay(hyper_replay_size)\n",
        "  agent = DdqnAgent(env, buffer)\n",
        "  epsilon = hyper_epsilon_start\n",
        "  optimizer = optim.Adam(net.parameters(), lr=hyper_gradient_step)\n",
        "  total_rewards = []\n",
        "  step_list = []\n",
        "  Eps = []\n",
        "  loss_episode = []\n",
        "\n",
        "  while True:    \n",
        "    reward = agent.play_step(net, epsilon,)\n",
        "    if len(buffer) > hyper_replay_start and reward is not None:\n",
        "      break\n",
        "\n",
        "  for episode_index in range(1,hyper_episodes+1):\n",
        "    step_index = 0\n",
        "    loss_step = []\n",
        "    epsilon = max(hyper_epsilon_final, hyper_epsilon_start * (hyper_epsilon_decay ** episode_index) )\n",
        "    Eps.append(epsilon)\n",
        "\n",
        "    while True:\n",
        "      step_index += 1\n",
        "      reward = agent.play_step(net, epsilon)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      batch = buffer.sample(hyper_batch_size)\n",
        "      loss_t = loss_calculation(batch, net, target_net)\n",
        "      loss_step.append(loss_t.item())\n",
        "      loss_t.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      temp_net = collections.OrderedDict()\n",
        "      for param_tensor in net.state_dict():\n",
        "        temp_net_new = net.state_dict()[param_tensor].numpy()\n",
        "        temp_net_old = target_net.state_dict()[param_tensor].numpy() \n",
        "        temp_net[param_tensor] = torch.from_numpy(np.array(temp_net_new * (1-hyper_soft_update) + temp_net_old * hyper_soft_update))\n",
        "      target_net.load_state_dict(temp_net)\n",
        "\n",
        "      if reward is not None:\n",
        "        step_list.append(step_index)\n",
        "        total_rewards.append(reward)\n",
        "        loss_episode.append(np.mean(np.array(loss_step)))\n",
        "        print(\"episode: %d/%d, step: %d, reward: %.2f, epsilon: %.2f\" % (episode_index, hyper_episodes, step_index, reward, epsilon))\n",
        "        break\n",
        "\n",
        "  plt.plot(list(range(1,hyper_episodes+1)),step_list)\n",
        "  plt.title('Steps of each Episode')\n",
        "  plt.ylabel('Steps')\n",
        "  plt.xlabel('Episodes')\n",
        "  plt.show()\n",
        "  plt.plot(list(range(1,hyper_episodes+1)),Eps)\n",
        "  plt.title('Epsilon of each Episode')\n",
        "  plt.ylabel('Epsilon')\n",
        "  plt.xlabel('Episodes')\n",
        "  plt.show()\n",
        "  plt.plot(list(range(1,hyper_episodes+1)),total_rewards)\n",
        "  plt.title('Reward of each Episode')\n",
        "  plt.ylabel('Reward')\n",
        "  plt.xlabel('Episodes')\n",
        "  plt.show()\n",
        "  plt.plot(list(range(1,hyper_episodes+1)),loss_episode)\n",
        "  plt.title('Mean Loss of each Episode')\n",
        "  plt.ylabel('Mean Loss')\n",
        "  plt.xlabel('Episodes')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 1/1000, step: 73, reward: 1.00, epsilon: 0.99\n",
            "episode: 2/1000, step: 324, reward: 0.00, epsilon: 0.98\n",
            "episode: 3/1000, step: 61, reward: 0.00, epsilon: 0.97\n",
            "episode: 4/1000, step: 204, reward: 0.00, epsilon: 0.96\n",
            "episode: 5/1000, step: 66, reward: 0.00, epsilon: 0.95\n",
            "episode: 6/1000, step: 77, reward: 0.00, epsilon: 0.94\n",
            "episode: 7/1000, step: 33, reward: 0.00, epsilon: 0.93\n",
            "episode: 8/1000, step: 68, reward: 0.00, epsilon: 0.92\n",
            "episode: 9/1000, step: 169, reward: 0.00, epsilon: 0.91\n",
            "episode: 10/1000, step: 77, reward: 0.00, epsilon: 0.90\n",
            "episode: 11/1000, step: 112, reward: 0.00, epsilon: 0.90\n",
            "episode: 12/1000, step: 112, reward: 0.00, epsilon: 0.89\n",
            "episode: 13/1000, step: 62, reward: 0.00, epsilon: 0.88\n",
            "episode: 14/1000, step: 61, reward: 0.00, epsilon: 0.87\n",
            "episode: 15/1000, step: 302, reward: 0.00, epsilon: 0.86\n",
            "episode: 16/1000, step: 43, reward: 0.00, epsilon: 0.85\n",
            "episode: 17/1000, step: 104, reward: 0.00, epsilon: 0.84\n",
            "episode: 18/1000, step: 177, reward: 0.00, epsilon: 0.83\n",
            "episode: 19/1000, step: 193, reward: 0.00, epsilon: 0.83\n",
            "episode: 20/1000, step: 100, reward: 0.00, epsilon: 0.82\n",
            "episode: 21/1000, step: 138, reward: 0.00, epsilon: 0.81\n",
            "episode: 22/1000, step: 125, reward: 0.00, epsilon: 0.80\n",
            "episode: 23/1000, step: 269, reward: -1.00, epsilon: 0.79\n",
            "episode: 24/1000, step: 113, reward: 0.00, epsilon: 0.79\n",
            "episode: 25/1000, step: 123, reward: 0.00, epsilon: 0.78\n",
            "episode: 26/1000, step: 130, reward: 0.00, epsilon: 0.77\n",
            "episode: 27/1000, step: 56, reward: 0.00, epsilon: 0.76\n",
            "episode: 28/1000, step: 117, reward: 0.00, epsilon: 0.75\n",
            "episode: 29/1000, step: 66, reward: 0.00, epsilon: 0.75\n",
            "episode: 30/1000, step: 180, reward: 0.00, epsilon: 0.74\n",
            "episode: 31/1000, step: 51, reward: 0.00, epsilon: 0.73\n",
            "episode: 32/1000, step: 400, reward: 0.00, epsilon: 0.72\n",
            "episode: 33/1000, step: 125, reward: 0.00, epsilon: 0.72\n",
            "episode: 34/1000, step: 123, reward: 0.00, epsilon: 0.71\n",
            "episode: 35/1000, step: 56, reward: 0.00, epsilon: 0.70\n",
            "episode: 36/1000, step: 187, reward: 0.00, epsilon: 0.70\n",
            "episode: 37/1000, step: 331, reward: 0.00, epsilon: 0.69\n",
            "episode: 38/1000, step: 120, reward: 0.00, epsilon: 0.68\n",
            "episode: 39/1000, step: 79, reward: -1.00, epsilon: 0.68\n",
            "episode: 40/1000, step: 75, reward: 0.00, epsilon: 0.67\n",
            "episode: 41/1000, step: 84, reward: 0.00, epsilon: 0.66\n",
            "episode: 42/1000, step: 142, reward: 0.00, epsilon: 0.66\n",
            "episode: 43/1000, step: 111, reward: 0.00, epsilon: 0.65\n",
            "episode: 44/1000, step: 195, reward: -1.00, epsilon: 0.64\n",
            "episode: 45/1000, step: 102, reward: 0.00, epsilon: 0.64\n",
            "episode: 46/1000, step: 400, reward: 0.00, epsilon: 0.63\n",
            "episode: 47/1000, step: 54, reward: 0.00, epsilon: 0.62\n",
            "episode: 48/1000, step: 104, reward: 0.00, epsilon: 0.62\n",
            "episode: 49/1000, step: 125, reward: 0.00, epsilon: 0.61\n",
            "episode: 50/1000, step: 54, reward: 0.00, epsilon: 0.61\n",
            "episode: 51/1000, step: 204, reward: 0.00, epsilon: 0.60\n",
            "episode: 52/1000, step: 61, reward: 0.00, epsilon: 0.59\n",
            "episode: 53/1000, step: 197, reward: 0.00, epsilon: 0.59\n",
            "episode: 54/1000, step: 63, reward: 0.00, epsilon: 0.58\n",
            "episode: 55/1000, step: 400, reward: 0.00, epsilon: 0.58\n",
            "episode: 56/1000, step: 95, reward: 0.00, epsilon: 0.57\n",
            "episode: 57/1000, step: 249, reward: -1.00, epsilon: 0.56\n",
            "episode: 58/1000, step: 59, reward: 0.00, epsilon: 0.56\n",
            "episode: 59/1000, step: 130, reward: 0.00, epsilon: 0.55\n",
            "episode: 60/1000, step: 56, reward: 0.00, epsilon: 0.55\n",
            "episode: 61/1000, step: 163, reward: 0.00, epsilon: 0.54\n",
            "episode: 62/1000, step: 111, reward: 1.00, epsilon: 0.54\n",
            "episode: 63/1000, step: 128, reward: -1.00, epsilon: 0.53\n",
            "episode: 64/1000, step: 178, reward: 0.00, epsilon: 0.53\n",
            "episode: 65/1000, step: 82, reward: 0.00, epsilon: 0.52\n",
            "episode: 66/1000, step: 400, reward: 0.00, epsilon: 0.52\n",
            "episode: 67/1000, step: 203, reward: 0.00, epsilon: 0.51\n",
            "episode: 68/1000, step: 36, reward: 0.00, epsilon: 0.50\n",
            "episode: 69/1000, step: 52, reward: 0.00, epsilon: 0.50\n",
            "episode: 70/1000, step: 121, reward: 0.00, epsilon: 0.49\n",
            "episode: 71/1000, step: 52, reward: 0.00, epsilon: 0.49\n",
            "episode: 72/1000, step: 123, reward: 0.00, epsilon: 0.48\n",
            "episode: 73/1000, step: 129, reward: -1.00, epsilon: 0.48\n",
            "episode: 74/1000, step: 96, reward: 0.00, epsilon: 0.48\n",
            "episode: 75/1000, step: 200, reward: 0.00, epsilon: 0.47\n",
            "episode: 76/1000, step: 121, reward: 0.00, epsilon: 0.47\n",
            "episode: 77/1000, step: 42, reward: 0.00, epsilon: 0.46\n",
            "episode: 78/1000, step: 98, reward: 0.00, epsilon: 0.46\n",
            "episode: 79/1000, step: 133, reward: 0.00, epsilon: 0.45\n",
            "episode: 80/1000, step: 131, reward: 0.00, epsilon: 0.45\n",
            "episode: 81/1000, step: 76, reward: -1.00, epsilon: 0.44\n",
            "episode: 82/1000, step: 108, reward: 0.00, epsilon: 0.44\n",
            "episode: 83/1000, step: 83, reward: 0.00, epsilon: 0.43\n",
            "episode: 84/1000, step: 101, reward: 0.00, epsilon: 0.43\n",
            "episode: 85/1000, step: 116, reward: 0.00, epsilon: 0.43\n",
            "episode: 86/1000, step: 44, reward: 0.00, epsilon: 0.42\n",
            "episode: 87/1000, step: 53, reward: 0.00, epsilon: 0.42\n",
            "episode: 88/1000, step: 75, reward: 0.00, epsilon: 0.41\n",
            "episode: 89/1000, step: 127, reward: 0.00, epsilon: 0.41\n",
            "episode: 90/1000, step: 400, reward: 0.00, epsilon: 0.40\n",
            "episode: 91/1000, step: 103, reward: 0.00, epsilon: 0.40\n",
            "episode: 92/1000, step: 109, reward: 0.00, epsilon: 0.40\n",
            "episode: 93/1000, step: 66, reward: 0.00, epsilon: 0.39\n",
            "episode: 94/1000, step: 244, reward: 0.00, epsilon: 0.39\n",
            "episode: 95/1000, step: 127, reward: 0.00, epsilon: 0.38\n",
            "episode: 96/1000, step: 400, reward: 0.00, epsilon: 0.38\n",
            "episode: 97/1000, step: 219, reward: 0.00, epsilon: 0.38\n",
            "episode: 98/1000, step: 400, reward: 0.00, epsilon: 0.37\n",
            "episode: 99/1000, step: 197, reward: 0.00, epsilon: 0.37\n",
            "episode: 100/1000, step: 97, reward: 0.00, epsilon: 0.37\n",
            "episode: 101/1000, step: 133, reward: 0.00, epsilon: 0.36\n",
            "episode: 102/1000, step: 54, reward: 0.00, epsilon: 0.36\n",
            "episode: 103/1000, step: 133, reward: 0.00, epsilon: 0.36\n",
            "episode: 104/1000, step: 400, reward: 0.00, epsilon: 0.35\n",
            "episode: 105/1000, step: 400, reward: 0.00, epsilon: 0.35\n",
            "episode: 106/1000, step: 372, reward: 0.00, epsilon: 0.34\n",
            "episode: 107/1000, step: 101, reward: 1.00, epsilon: 0.34\n",
            "episode: 108/1000, step: 273, reward: 0.00, epsilon: 0.34\n",
            "episode: 109/1000, step: 133, reward: 0.00, epsilon: 0.33\n",
            "episode: 110/1000, step: 81, reward: 0.00, epsilon: 0.33\n",
            "episode: 111/1000, step: 73, reward: 1.00, epsilon: 0.33\n",
            "episode: 112/1000, step: 91, reward: 1.00, epsilon: 0.32\n",
            "episode: 113/1000, step: 122, reward: 0.00, epsilon: 0.32\n",
            "episode: 114/1000, step: 124, reward: 0.00, epsilon: 0.32\n",
            "episode: 115/1000, step: 121, reward: 0.00, epsilon: 0.31\n",
            "episode: 116/1000, step: 251, reward: 0.00, epsilon: 0.31\n",
            "episode: 117/1000, step: 92, reward: 0.00, epsilon: 0.31\n",
            "episode: 118/1000, step: 131, reward: 0.00, epsilon: 0.31\n",
            "episode: 119/1000, step: 115, reward: 0.00, epsilon: 0.30\n",
            "episode: 120/1000, step: 70, reward: 0.00, epsilon: 0.30\n",
            "episode: 121/1000, step: 65, reward: 0.00, epsilon: 0.30\n",
            "episode: 122/1000, step: 104, reward: 0.00, epsilon: 0.29\n",
            "episode: 123/1000, step: 270, reward: 0.00, epsilon: 0.29\n",
            "episode: 124/1000, step: 69, reward: 0.00, epsilon: 0.29\n",
            "episode: 125/1000, step: 89, reward: 0.00, epsilon: 0.28\n",
            "episode: 126/1000, step: 246, reward: 0.00, epsilon: 0.28\n",
            "episode: 127/1000, step: 43, reward: 0.00, epsilon: 0.28\n",
            "episode: 128/1000, step: 75, reward: 0.00, epsilon: 0.28\n",
            "episode: 129/1000, step: 69, reward: 0.00, epsilon: 0.27\n",
            "episode: 130/1000, step: 70, reward: -1.00, epsilon: 0.27\n",
            "episode: 131/1000, step: 252, reward: 0.00, epsilon: 0.27\n",
            "episode: 132/1000, step: 139, reward: 0.00, epsilon: 0.27\n",
            "episode: 133/1000, step: 50, reward: 0.00, epsilon: 0.26\n",
            "episode: 134/1000, step: 136, reward: 0.00, epsilon: 0.26\n",
            "episode: 135/1000, step: 90, reward: 1.00, epsilon: 0.26\n",
            "episode: 136/1000, step: 91, reward: 0.00, epsilon: 0.25\n",
            "episode: 137/1000, step: 66, reward: 0.00, epsilon: 0.25\n",
            "episode: 138/1000, step: 58, reward: 0.00, epsilon: 0.25\n",
            "episode: 139/1000, step: 137, reward: 0.00, epsilon: 0.25\n",
            "episode: 140/1000, step: 56, reward: 0.00, epsilon: 0.24\n",
            "episode: 141/1000, step: 51, reward: 0.00, epsilon: 0.24\n",
            "episode: 142/1000, step: 224, reward: 0.00, epsilon: 0.24\n",
            "episode: 143/1000, step: 202, reward: 0.00, epsilon: 0.24\n",
            "episode: 144/1000, step: 38, reward: 0.00, epsilon: 0.24\n",
            "episode: 145/1000, step: 64, reward: 0.00, epsilon: 0.23\n",
            "episode: 146/1000, step: 140, reward: 0.00, epsilon: 0.23\n",
            "episode: 147/1000, step: 330, reward: 0.00, epsilon: 0.23\n",
            "episode: 148/1000, step: 172, reward: 0.00, epsilon: 0.23\n",
            "episode: 149/1000, step: 79, reward: 1.00, epsilon: 0.22\n",
            "episode: 150/1000, step: 119, reward: 0.00, epsilon: 0.22\n",
            "episode: 151/1000, step: 53, reward: -1.00, epsilon: 0.22\n",
            "episode: 152/1000, step: 121, reward: 0.00, epsilon: 0.22\n",
            "episode: 153/1000, step: 64, reward: 0.00, epsilon: 0.21\n",
            "episode: 154/1000, step: 246, reward: 0.00, epsilon: 0.21\n",
            "episode: 155/1000, step: 54, reward: 0.00, epsilon: 0.21\n",
            "episode: 156/1000, step: 400, reward: 0.00, epsilon: 0.21\n",
            "episode: 157/1000, step: 122, reward: 0.00, epsilon: 0.21\n",
            "episode: 158/1000, step: 57, reward: 0.00, epsilon: 0.20\n",
            "episode: 159/1000, step: 132, reward: 0.00, epsilon: 0.20\n",
            "episode: 160/1000, step: 60, reward: 0.00, epsilon: 0.20\n",
            "episode: 161/1000, step: 226, reward: 0.00, epsilon: 0.20\n",
            "episode: 162/1000, step: 167, reward: -1.00, epsilon: 0.20\n",
            "episode: 163/1000, step: 69, reward: 0.00, epsilon: 0.19\n",
            "episode: 164/1000, step: 50, reward: 0.00, epsilon: 0.19\n",
            "episode: 165/1000, step: 50, reward: -1.00, epsilon: 0.19\n",
            "episode: 166/1000, step: 52, reward: -1.00, epsilon: 0.19\n",
            "episode: 167/1000, step: 126, reward: 0.00, epsilon: 0.19\n",
            "episode: 168/1000, step: 111, reward: 0.00, epsilon: 0.18\n",
            "episode: 169/1000, step: 45, reward: 0.00, epsilon: 0.18\n",
            "episode: 170/1000, step: 109, reward: 0.00, epsilon: 0.18\n",
            "episode: 171/1000, step: 101, reward: 0.00, epsilon: 0.18\n",
            "episode: 172/1000, step: 94, reward: 0.00, epsilon: 0.18\n",
            "episode: 173/1000, step: 62, reward: 0.00, epsilon: 0.18\n",
            "episode: 174/1000, step: 52, reward: 0.00, epsilon: 0.17\n",
            "episode: 175/1000, step: 400, reward: 0.00, epsilon: 0.17\n",
            "episode: 176/1000, step: 348, reward: 0.00, epsilon: 0.17\n",
            "episode: 177/1000, step: 47, reward: 0.00, epsilon: 0.17\n",
            "episode: 178/1000, step: 69, reward: 0.00, epsilon: 0.17\n",
            "episode: 179/1000, step: 117, reward: 0.00, epsilon: 0.17\n",
            "episode: 180/1000, step: 81, reward: 0.00, epsilon: 0.16\n",
            "episode: 181/1000, step: 141, reward: 0.00, epsilon: 0.16\n",
            "episode: 182/1000, step: 67, reward: 0.00, epsilon: 0.16\n",
            "episode: 183/1000, step: 86, reward: -1.00, epsilon: 0.16\n",
            "episode: 184/1000, step: 125, reward: 0.00, epsilon: 0.16\n",
            "episode: 185/1000, step: 168, reward: 0.00, epsilon: 0.16\n",
            "episode: 186/1000, step: 112, reward: 0.00, epsilon: 0.15\n",
            "episode: 187/1000, step: 96, reward: 0.00, epsilon: 0.15\n",
            "episode: 188/1000, step: 55, reward: 0.00, epsilon: 0.15\n",
            "episode: 189/1000, step: 77, reward: 0.00, epsilon: 0.15\n",
            "episode: 190/1000, step: 196, reward: 0.00, epsilon: 0.15\n",
            "episode: 191/1000, step: 95, reward: 0.00, epsilon: 0.15\n",
            "episode: 192/1000, step: 84, reward: 0.00, epsilon: 0.15\n",
            "episode: 193/1000, step: 130, reward: 0.00, epsilon: 0.14\n",
            "episode: 194/1000, step: 58, reward: 0.00, epsilon: 0.14\n",
            "episode: 195/1000, step: 108, reward: 0.00, epsilon: 0.14\n",
            "episode: 196/1000, step: 104, reward: 1.00, epsilon: 0.14\n",
            "episode: 197/1000, step: 83, reward: 0.00, epsilon: 0.14\n",
            "episode: 198/1000, step: 280, reward: 0.00, epsilon: 0.14\n",
            "episode: 199/1000, step: 400, reward: 0.00, epsilon: 0.14\n",
            "episode: 200/1000, step: 135, reward: 0.00, epsilon: 0.13\n",
            "episode: 201/1000, step: 145, reward: 0.00, epsilon: 0.13\n",
            "episode: 202/1000, step: 127, reward: 0.00, epsilon: 0.13\n",
            "episode: 203/1000, step: 400, reward: 0.00, epsilon: 0.13\n",
            "episode: 204/1000, step: 150, reward: 1.00, epsilon: 0.13\n",
            "episode: 205/1000, step: 87, reward: 0.00, epsilon: 0.13\n",
            "episode: 206/1000, step: 298, reward: 0.00, epsilon: 0.13\n",
            "episode: 207/1000, step: 117, reward: 0.00, epsilon: 0.12\n",
            "episode: 208/1000, step: 95, reward: 0.00, epsilon: 0.12\n",
            "episode: 209/1000, step: 146, reward: 0.00, epsilon: 0.12\n",
            "episode: 210/1000, step: 111, reward: 0.00, epsilon: 0.12\n",
            "episode: 211/1000, step: 94, reward: 0.00, epsilon: 0.12\n",
            "episode: 212/1000, step: 69, reward: 0.00, epsilon: 0.12\n",
            "episode: 213/1000, step: 105, reward: 0.00, epsilon: 0.12\n",
            "episode: 214/1000, step: 90, reward: 0.00, epsilon: 0.12\n",
            "episode: 215/1000, step: 39, reward: 0.00, epsilon: 0.12\n",
            "episode: 216/1000, step: 256, reward: 0.00, epsilon: 0.11\n",
            "episode: 217/1000, step: 186, reward: 0.00, epsilon: 0.11\n",
            "episode: 218/1000, step: 357, reward: 0.00, epsilon: 0.11\n",
            "episode: 219/1000, step: 254, reward: 0.00, epsilon: 0.11\n",
            "episode: 220/1000, step: 400, reward: 0.00, epsilon: 0.11\n",
            "episode: 221/1000, step: 93, reward: 0.00, epsilon: 0.11\n",
            "episode: 222/1000, step: 98, reward: 0.00, epsilon: 0.11\n",
            "episode: 223/1000, step: 225, reward: 0.00, epsilon: 0.11\n",
            "episode: 224/1000, step: 85, reward: 0.00, epsilon: 0.11\n",
            "episode: 225/1000, step: 146, reward: 0.00, epsilon: 0.10\n",
            "episode: 226/1000, step: 99, reward: 0.00, epsilon: 0.10\n",
            "episode: 227/1000, step: 54, reward: 0.00, epsilon: 0.10\n",
            "episode: 228/1000, step: 131, reward: 0.00, epsilon: 0.10\n",
            "episode: 229/1000, step: 69, reward: 0.00, epsilon: 0.10\n",
            "episode: 230/1000, step: 74, reward: 0.00, epsilon: 0.10\n",
            "episode: 231/1000, step: 400, reward: 0.00, epsilon: 0.10\n",
            "episode: 232/1000, step: 60, reward: 0.00, epsilon: 0.10\n",
            "episode: 233/1000, step: 51, reward: 0.00, epsilon: 0.10\n",
            "episode: 234/1000, step: 216, reward: 0.00, epsilon: 0.10\n",
            "episode: 235/1000, step: 95, reward: 0.00, epsilon: 0.09\n",
            "episode: 236/1000, step: 59, reward: 0.00, epsilon: 0.09\n",
            "episode: 237/1000, step: 114, reward: 0.00, epsilon: 0.09\n",
            "episode: 238/1000, step: 137, reward: -1.00, epsilon: 0.09\n",
            "episode: 239/1000, step: 123, reward: 0.00, epsilon: 0.09\n",
            "episode: 240/1000, step: 74, reward: 0.00, epsilon: 0.09\n",
            "episode: 241/1000, step: 400, reward: 0.00, epsilon: 0.09\n",
            "episode: 242/1000, step: 400, reward: 0.00, epsilon: 0.09\n",
            "episode: 243/1000, step: 273, reward: 0.00, epsilon: 0.09\n",
            "episode: 244/1000, step: 77, reward: 0.00, epsilon: 0.09\n",
            "episode: 245/1000, step: 104, reward: 0.00, epsilon: 0.09\n",
            "episode: 246/1000, step: 124, reward: 0.00, epsilon: 0.08\n",
            "episode: 247/1000, step: 400, reward: 0.00, epsilon: 0.08\n",
            "episode: 248/1000, step: 222, reward: 0.00, epsilon: 0.08\n",
            "episode: 249/1000, step: 179, reward: 0.00, epsilon: 0.08\n",
            "episode: 250/1000, step: 77, reward: 1.00, epsilon: 0.08\n",
            "episode: 251/1000, step: 92, reward: 0.00, epsilon: 0.08\n",
            "episode: 252/1000, step: 226, reward: 0.00, epsilon: 0.08\n",
            "episode: 253/1000, step: 51, reward: 0.00, epsilon: 0.08\n",
            "episode: 254/1000, step: 159, reward: 0.00, epsilon: 0.08\n",
            "episode: 255/1000, step: 80, reward: 0.00, epsilon: 0.08\n",
            "episode: 256/1000, step: 122, reward: 0.00, epsilon: 0.08\n",
            "episode: 257/1000, step: 106, reward: 0.00, epsilon: 0.08\n",
            "episode: 258/1000, step: 262, reward: 0.00, epsilon: 0.07\n",
            "episode: 259/1000, step: 92, reward: 0.00, epsilon: 0.07\n",
            "episode: 260/1000, step: 245, reward: 0.00, epsilon: 0.07\n",
            "episode: 261/1000, step: 288, reward: 0.00, epsilon: 0.07\n",
            "episode: 262/1000, step: 74, reward: 0.00, epsilon: 0.07\n",
            "episode: 263/1000, step: 354, reward: 0.00, epsilon: 0.07\n",
            "episode: 264/1000, step: 217, reward: 0.00, epsilon: 0.07\n",
            "episode: 265/1000, step: 335, reward: 0.00, epsilon: 0.07\n",
            "episode: 266/1000, step: 370, reward: 0.00, epsilon: 0.07\n",
            "episode: 267/1000, step: 68, reward: 0.00, epsilon: 0.07\n",
            "episode: 268/1000, step: 82, reward: 0.00, epsilon: 0.07\n",
            "episode: 269/1000, step: 133, reward: 0.00, epsilon: 0.07\n",
            "episode: 270/1000, step: 286, reward: 0.00, epsilon: 0.07\n",
            "episode: 271/1000, step: 104, reward: 0.00, epsilon: 0.07\n",
            "episode: 272/1000, step: 85, reward: 0.00, epsilon: 0.06\n",
            "episode: 273/1000, step: 65, reward: 0.00, epsilon: 0.06\n",
            "episode: 274/1000, step: 88, reward: 0.00, epsilon: 0.06\n",
            "episode: 275/1000, step: 143, reward: 0.00, epsilon: 0.06\n",
            "episode: 276/1000, step: 60, reward: 0.00, epsilon: 0.06\n",
            "episode: 277/1000, step: 88, reward: 0.00, epsilon: 0.06\n",
            "episode: 278/1000, step: 79, reward: 0.00, epsilon: 0.06\n",
            "episode: 279/1000, step: 72, reward: 0.00, epsilon: 0.06\n",
            "episode: 280/1000, step: 82, reward: 0.00, epsilon: 0.06\n",
            "episode: 281/1000, step: 145, reward: 0.00, epsilon: 0.06\n",
            "episode: 282/1000, step: 110, reward: 0.00, epsilon: 0.06\n",
            "episode: 283/1000, step: 400, reward: 0.00, epsilon: 0.06\n",
            "episode: 284/1000, step: 79, reward: 0.00, epsilon: 0.06\n",
            "episode: 285/1000, step: 284, reward: 0.00, epsilon: 0.06\n",
            "episode: 286/1000, step: 254, reward: 0.00, epsilon: 0.06\n",
            "episode: 287/1000, step: 107, reward: 0.00, epsilon: 0.06\n",
            "episode: 288/1000, step: 75, reward: 0.00, epsilon: 0.06\n",
            "episode: 289/1000, step: 236, reward: 0.00, epsilon: 0.05\n",
            "episode: 290/1000, step: 324, reward: 0.00, epsilon: 0.05\n",
            "episode: 291/1000, step: 152, reward: 0.00, epsilon: 0.05\n",
            "episode: 292/1000, step: 70, reward: 0.00, epsilon: 0.05\n",
            "episode: 293/1000, step: 252, reward: 0.00, epsilon: 0.05\n",
            "episode: 294/1000, step: 270, reward: 0.00, epsilon: 0.05\n",
            "episode: 295/1000, step: 189, reward: 0.00, epsilon: 0.05\n",
            "episode: 296/1000, step: 58, reward: 0.00, epsilon: 0.05\n",
            "episode: 297/1000, step: 123, reward: -1.00, epsilon: 0.05\n",
            "episode: 298/1000, step: 117, reward: 0.00, epsilon: 0.05\n",
            "episode: 299/1000, step: 91, reward: 0.00, epsilon: 0.05\n",
            "episode: 300/1000, step: 61, reward: 0.00, epsilon: 0.05\n",
            "episode: 301/1000, step: 105, reward: 0.00, epsilon: 0.05\n",
            "episode: 302/1000, step: 133, reward: 0.00, epsilon: 0.05\n",
            "episode: 303/1000, step: 141, reward: 0.00, epsilon: 0.05\n",
            "episode: 304/1000, step: 314, reward: 0.00, epsilon: 0.05\n",
            "episode: 305/1000, step: 130, reward: 0.00, epsilon: 0.05\n",
            "episode: 306/1000, step: 98, reward: 0.00, epsilon: 0.05\n",
            "episode: 307/1000, step: 123, reward: 0.00, epsilon: 0.05\n",
            "episode: 308/1000, step: 84, reward: 0.00, epsilon: 0.05\n",
            "episode: 309/1000, step: 101, reward: 0.00, epsilon: 0.04\n",
            "episode: 310/1000, step: 149, reward: 0.00, epsilon: 0.04\n",
            "episode: 311/1000, step: 356, reward: 0.00, epsilon: 0.04\n",
            "episode: 312/1000, step: 247, reward: 0.00, epsilon: 0.04\n",
            "episode: 313/1000, step: 86, reward: 0.00, epsilon: 0.04\n",
            "episode: 314/1000, step: 371, reward: 0.00, epsilon: 0.04\n",
            "episode: 315/1000, step: 154, reward: 0.00, epsilon: 0.04\n",
            "episode: 316/1000, step: 400, reward: 0.00, epsilon: 0.04\n",
            "episode: 317/1000, step: 161, reward: 0.00, epsilon: 0.04\n",
            "episode: 318/1000, step: 173, reward: 0.00, epsilon: 0.04\n",
            "episode: 319/1000, step: 400, reward: 0.00, epsilon: 0.04\n",
            "episode: 320/1000, step: 85, reward: 0.00, epsilon: 0.04\n",
            "episode: 321/1000, step: 58, reward: 0.00, epsilon: 0.04\n",
            "episode: 322/1000, step: 400, reward: 0.00, epsilon: 0.04\n",
            "episode: 323/1000, step: 117, reward: 0.00, epsilon: 0.04\n",
            "episode: 324/1000, step: 213, reward: 0.00, epsilon: 0.04\n",
            "episode: 325/1000, step: 79, reward: 0.00, epsilon: 0.04\n",
            "episode: 326/1000, step: 95, reward: -1.00, epsilon: 0.04\n",
            "episode: 327/1000, step: 161, reward: 0.00, epsilon: 0.04\n",
            "episode: 328/1000, step: 110, reward: 0.00, epsilon: 0.04\n",
            "episode: 329/1000, step: 84, reward: 0.00, epsilon: 0.04\n",
            "episode: 330/1000, step: 359, reward: 0.00, epsilon: 0.04\n",
            "episode: 331/1000, step: 45, reward: 0.00, epsilon: 0.04\n",
            "episode: 332/1000, step: 139, reward: -1.00, epsilon: 0.04\n",
            "episode: 333/1000, step: 83, reward: 0.00, epsilon: 0.04\n",
            "episode: 334/1000, step: 75, reward: 1.00, epsilon: 0.03\n",
            "episode: 335/1000, step: 61, reward: 0.00, epsilon: 0.03\n",
            "episode: 336/1000, step: 77, reward: 1.00, epsilon: 0.03\n",
            "episode: 337/1000, step: 75, reward: 1.00, epsilon: 0.03\n",
            "episode: 338/1000, step: 71, reward: 1.00, epsilon: 0.03\n",
            "episode: 339/1000, step: 67, reward: 1.00, epsilon: 0.03\n",
            "episode: 340/1000, step: 149, reward: 0.00, epsilon: 0.03\n",
            "episode: 341/1000, step: 178, reward: 0.00, epsilon: 0.03\n",
            "episode: 342/1000, step: 170, reward: 0.00, epsilon: 0.03\n",
            "episode: 343/1000, step: 400, reward: 0.00, epsilon: 0.03\n",
            "episode: 344/1000, step: 73, reward: 1.00, epsilon: 0.03\n",
            "episode: 345/1000, step: 71, reward: 1.00, epsilon: 0.03\n",
            "episode: 346/1000, step: 102, reward: 0.00, epsilon: 0.03\n",
            "episode: 347/1000, step: 73, reward: 0.00, epsilon: 0.03\n",
            "episode: 348/1000, step: 128, reward: 0.00, epsilon: 0.03\n",
            "episode: 349/1000, step: 400, reward: 0.00, epsilon: 0.03\n",
            "episode: 350/1000, step: 400, reward: 0.00, epsilon: 0.03\n",
            "episode: 351/1000, step: 62, reward: 0.00, epsilon: 0.03\n",
            "episode: 352/1000, step: 93, reward: 0.00, epsilon: 0.03\n",
            "episode: 353/1000, step: 72, reward: 0.00, epsilon: 0.03\n",
            "episode: 354/1000, step: 400, reward: 0.00, epsilon: 0.03\n",
            "episode: 355/1000, step: 77, reward: 0.00, epsilon: 0.03\n",
            "episode: 356/1000, step: 73, reward: 0.00, epsilon: 0.03\n",
            "episode: 357/1000, step: 197, reward: 0.00, epsilon: 0.03\n",
            "episode: 358/1000, step: 109, reward: 0.00, epsilon: 0.03\n",
            "episode: 359/1000, step: 179, reward: 0.00, epsilon: 0.03\n",
            "episode: 360/1000, step: 285, reward: 0.00, epsilon: 0.03\n",
            "episode: 361/1000, step: 144, reward: 0.00, epsilon: 0.03\n",
            "episode: 362/1000, step: 195, reward: 0.00, epsilon: 0.03\n",
            "episode: 363/1000, step: 268, reward: 0.00, epsilon: 0.03\n",
            "episode: 364/1000, step: 130, reward: 0.00, epsilon: 0.03\n",
            "episode: 365/1000, step: 185, reward: 0.00, epsilon: 0.03\n",
            "episode: 366/1000, step: 153, reward: 0.00, epsilon: 0.03\n",
            "episode: 367/1000, step: 51, reward: 0.00, epsilon: 0.03\n",
            "episode: 368/1000, step: 103, reward: 0.00, epsilon: 0.02\n",
            "episode: 369/1000, step: 158, reward: 0.00, epsilon: 0.02\n",
            "episode: 370/1000, step: 79, reward: 0.00, epsilon: 0.02\n",
            "episode: 371/1000, step: 400, reward: 0.00, epsilon: 0.02\n",
            "episode: 372/1000, step: 167, reward: 0.00, epsilon: 0.02\n",
            "episode: 373/1000, step: 400, reward: 0.00, epsilon: 0.02\n",
            "episode: 374/1000, step: 93, reward: 0.00, epsilon: 0.02\n",
            "episode: 375/1000, step: 129, reward: 0.00, epsilon: 0.02\n",
            "episode: 376/1000, step: 93, reward: 0.00, epsilon: 0.02\n",
            "episode: 377/1000, step: 75, reward: 0.00, epsilon: 0.02\n",
            "episode: 378/1000, step: 134, reward: 0.00, epsilon: 0.02\n",
            "episode: 379/1000, step: 51, reward: 0.00, epsilon: 0.02\n",
            "episode: 380/1000, step: 52, reward: 0.00, epsilon: 0.02\n",
            "episode: 381/1000, step: 309, reward: 0.00, epsilon: 0.02\n",
            "episode: 382/1000, step: 58, reward: 0.00, epsilon: 0.02\n",
            "episode: 383/1000, step: 286, reward: 0.00, epsilon: 0.02\n",
            "episode: 384/1000, step: 131, reward: 0.00, epsilon: 0.02\n",
            "episode: 385/1000, step: 222, reward: 0.00, epsilon: 0.02\n",
            "episode: 386/1000, step: 270, reward: -1.00, epsilon: 0.02\n",
            "episode: 387/1000, step: 400, reward: 0.00, epsilon: 0.02\n",
            "episode: 388/1000, step: 100, reward: 0.00, epsilon: 0.02\n",
            "episode: 389/1000, step: 400, reward: 0.00, epsilon: 0.02\n",
            "episode: 390/1000, step: 165, reward: 0.00, epsilon: 0.02\n",
            "episode: 391/1000, step: 97, reward: 0.00, epsilon: 0.02\n",
            "episode: 392/1000, step: 80, reward: 0.00, epsilon: 0.02\n",
            "episode: 393/1000, step: 183, reward: 0.00, epsilon: 0.02\n",
            "episode: 394/1000, step: 269, reward: 0.00, epsilon: 0.02\n",
            "episode: 395/1000, step: 268, reward: 0.00, epsilon: 0.02\n",
            "episode: 396/1000, step: 100, reward: 0.00, epsilon: 0.02\n",
            "episode: 397/1000, step: 339, reward: 0.00, epsilon: 0.02\n",
            "episode: 398/1000, step: 248, reward: 0.00, epsilon: 0.02\n",
            "episode: 399/1000, step: 261, reward: 0.00, epsilon: 0.02\n",
            "episode: 400/1000, step: 105, reward: 0.00, epsilon: 0.02\n",
            "episode: 401/1000, step: 100, reward: 0.00, epsilon: 0.02\n",
            "episode: 402/1000, step: 400, reward: 0.00, epsilon: 0.02\n",
            "episode: 403/1000, step: 215, reward: 0.00, epsilon: 0.02\n",
            "episode: 404/1000, step: 120, reward: 0.00, epsilon: 0.02\n",
            "episode: 405/1000, step: 101, reward: -1.00, epsilon: 0.02\n",
            "episode: 406/1000, step: 115, reward: -1.00, epsilon: 0.02\n",
            "episode: 407/1000, step: 178, reward: -1.00, epsilon: 0.02\n",
            "episode: 408/1000, step: 400, reward: 0.00, epsilon: 0.02\n",
            "episode: 409/1000, step: 114, reward: -1.00, epsilon: 0.02\n",
            "episode: 410/1000, step: 110, reward: 0.00, epsilon: 0.02\n",
            "episode: 411/1000, step: 100, reward: 0.00, epsilon: 0.02\n",
            "episode: 412/1000, step: 122, reward: 0.00, epsilon: 0.02\n",
            "episode: 413/1000, step: 371, reward: -1.00, epsilon: 0.02\n",
            "episode: 414/1000, step: 400, reward: 0.00, epsilon: 0.02\n",
            "episode: 415/1000, step: 400, reward: 0.00, epsilon: 0.02\n",
            "episode: 416/1000, step: 173, reward: 0.00, epsilon: 0.02\n",
            "episode: 417/1000, step: 62, reward: 0.00, epsilon: 0.02\n",
            "episode: 418/1000, step: 392, reward: 0.00, epsilon: 0.01\n",
            "episode: 419/1000, step: 123, reward: 0.00, epsilon: 0.01\n",
            "episode: 420/1000, step: 153, reward: 0.00, epsilon: 0.01\n",
            "episode: 421/1000, step: 241, reward: 0.00, epsilon: 0.01\n",
            "episode: 422/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 423/1000, step: 268, reward: 0.00, epsilon: 0.01\n",
            "episode: 424/1000, step: 381, reward: 0.00, epsilon: 0.01\n",
            "episode: 425/1000, step: 235, reward: 0.00, epsilon: 0.01\n",
            "episode: 426/1000, step: 118, reward: 0.00, epsilon: 0.01\n",
            "episode: 427/1000, step: 119, reward: 0.00, epsilon: 0.01\n",
            "episode: 428/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 429/1000, step: 214, reward: 0.00, epsilon: 0.01\n",
            "episode: 430/1000, step: 122, reward: 0.00, epsilon: 0.01\n",
            "episode: 431/1000, step: 95, reward: 0.00, epsilon: 0.01\n",
            "episode: 432/1000, step: 191, reward: 0.00, epsilon: 0.01\n",
            "episode: 433/1000, step: 149, reward: 0.00, epsilon: 0.01\n",
            "episode: 434/1000, step: 176, reward: 0.00, epsilon: 0.01\n",
            "episode: 435/1000, step: 159, reward: -1.00, epsilon: 0.01\n",
            "episode: 436/1000, step: 316, reward: 0.00, epsilon: 0.01\n",
            "episode: 437/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 438/1000, step: 142, reward: -1.00, epsilon: 0.01\n",
            "episode: 439/1000, step: 168, reward: 0.00, epsilon: 0.01\n",
            "episode: 440/1000, step: 294, reward: -1.00, epsilon: 0.01\n",
            "episode: 441/1000, step: 95, reward: 0.00, epsilon: 0.01\n",
            "episode: 442/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 443/1000, step: 227, reward: 0.00, epsilon: 0.01\n",
            "episode: 444/1000, step: 165, reward: 0.00, epsilon: 0.01\n",
            "episode: 445/1000, step: 176, reward: 0.00, epsilon: 0.01\n",
            "episode: 446/1000, step: 122, reward: 0.00, epsilon: 0.01\n",
            "episode: 447/1000, step: 61, reward: 0.00, epsilon: 0.01\n",
            "episode: 448/1000, step: 77, reward: -1.00, epsilon: 0.01\n",
            "episode: 449/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 450/1000, step: 72, reward: 0.00, epsilon: 0.01\n",
            "episode: 451/1000, step: 376, reward: 0.00, epsilon: 0.01\n",
            "episode: 452/1000, step: 62, reward: 0.00, epsilon: 0.01\n",
            "episode: 453/1000, step: 156, reward: 0.00, epsilon: 0.01\n",
            "episode: 454/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 455/1000, step: 170, reward: 0.00, epsilon: 0.01\n",
            "episode: 456/1000, step: 260, reward: 0.00, epsilon: 0.01\n",
            "episode: 457/1000, step: 278, reward: 0.00, epsilon: 0.01\n",
            "episode: 458/1000, step: 173, reward: -1.00, epsilon: 0.01\n",
            "episode: 459/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 460/1000, step: 160, reward: 0.00, epsilon: 0.01\n",
            "episode: 461/1000, step: 55, reward: 0.00, epsilon: 0.01\n",
            "episode: 462/1000, step: 96, reward: 0.00, epsilon: 0.01\n",
            "episode: 463/1000, step: 147, reward: -1.00, epsilon: 0.01\n",
            "episode: 464/1000, step: 265, reward: 0.00, epsilon: 0.01\n",
            "episode: 465/1000, step: 230, reward: -1.00, epsilon: 0.01\n",
            "episode: 466/1000, step: 112, reward: 0.00, epsilon: 0.01\n",
            "episode: 467/1000, step: 146, reward: 0.00, epsilon: 0.01\n",
            "episode: 468/1000, step: 59, reward: 0.00, epsilon: 0.01\n",
            "episode: 469/1000, step: 135, reward: 0.00, epsilon: 0.01\n",
            "episode: 470/1000, step: 153, reward: 0.00, epsilon: 0.01\n",
            "episode: 471/1000, step: 110, reward: 0.00, epsilon: 0.01\n",
            "episode: 472/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 473/1000, step: 157, reward: -1.00, epsilon: 0.01\n",
            "episode: 474/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 475/1000, step: 95, reward: 0.00, epsilon: 0.01\n",
            "episode: 476/1000, step: 227, reward: 0.00, epsilon: 0.01\n",
            "episode: 477/1000, step: 138, reward: 0.00, epsilon: 0.01\n",
            "episode: 478/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 479/1000, step: 168, reward: 0.00, epsilon: 0.01\n",
            "episode: 480/1000, step: 161, reward: -1.00, epsilon: 0.01\n",
            "episode: 481/1000, step: 247, reward: 0.00, epsilon: 0.01\n",
            "episode: 482/1000, step: 259, reward: 0.00, epsilon: 0.01\n",
            "episode: 483/1000, step: 233, reward: -1.00, epsilon: 0.01\n",
            "episode: 484/1000, step: 135, reward: 0.00, epsilon: 0.01\n",
            "episode: 485/1000, step: 168, reward: 0.00, epsilon: 0.01\n",
            "episode: 486/1000, step: 155, reward: 0.00, epsilon: 0.01\n",
            "episode: 487/1000, step: 53, reward: 0.00, epsilon: 0.01\n",
            "episode: 488/1000, step: 198, reward: 0.00, epsilon: 0.01\n",
            "episode: 489/1000, step: 375, reward: 0.00, epsilon: 0.01\n",
            "episode: 490/1000, step: 183, reward: 0.00, epsilon: 0.01\n",
            "episode: 491/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 492/1000, step: 123, reward: 0.00, epsilon: 0.01\n",
            "episode: 493/1000, step: 277, reward: 0.00, epsilon: 0.01\n",
            "episode: 494/1000, step: 60, reward: 0.00, epsilon: 0.01\n",
            "episode: 495/1000, step: 95, reward: 0.00, epsilon: 0.01\n",
            "episode: 496/1000, step: 45, reward: 0.00, epsilon: 0.01\n",
            "episode: 497/1000, step: 76, reward: 0.00, epsilon: 0.01\n",
            "episode: 498/1000, step: 61, reward: 0.00, epsilon: 0.01\n",
            "episode: 499/1000, step: 255, reward: -1.00, epsilon: 0.01\n",
            "episode: 500/1000, step: 132, reward: -1.00, epsilon: 0.01\n",
            "episode: 501/1000, step: 351, reward: 0.00, epsilon: 0.01\n",
            "episode: 502/1000, step: 303, reward: 0.00, epsilon: 0.01\n",
            "episode: 503/1000, step: 217, reward: 0.00, epsilon: 0.01\n",
            "episode: 504/1000, step: 275, reward: 0.00, epsilon: 0.01\n",
            "episode: 505/1000, step: 111, reward: 0.00, epsilon: 0.01\n",
            "episode: 506/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 507/1000, step: 176, reward: 0.00, epsilon: 0.01\n",
            "episode: 508/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 509/1000, step: 317, reward: 0.00, epsilon: 0.01\n",
            "episode: 510/1000, step: 302, reward: 0.00, epsilon: 0.01\n",
            "episode: 511/1000, step: 241, reward: 0.00, epsilon: 0.01\n",
            "episode: 512/1000, step: 61, reward: 0.00, epsilon: 0.01\n",
            "episode: 513/1000, step: 72, reward: 0.00, epsilon: 0.01\n",
            "episode: 514/1000, step: 391, reward: 0.00, epsilon: 0.01\n",
            "episode: 515/1000, step: 86, reward: 0.00, epsilon: 0.01\n",
            "episode: 516/1000, step: 135, reward: 0.00, epsilon: 0.01\n",
            "episode: 517/1000, step: 147, reward: 0.00, epsilon: 0.01\n",
            "episode: 518/1000, step: 77, reward: 0.00, epsilon: 0.01\n",
            "episode: 519/1000, step: 287, reward: 0.00, epsilon: 0.01\n",
            "episode: 520/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 521/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 522/1000, step: 77, reward: 0.00, epsilon: 0.01\n",
            "episode: 523/1000, step: 187, reward: 0.00, epsilon: 0.01\n",
            "episode: 524/1000, step: 393, reward: 0.00, epsilon: 0.01\n",
            "episode: 525/1000, step: 183, reward: 0.00, epsilon: 0.01\n",
            "episode: 526/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 527/1000, step: 400, reward: 0.00, epsilon: 0.01\n",
            "episode: 528/1000, step: 389, reward: 0.00, epsilon: 0.00\n",
            "episode: 529/1000, step: 222, reward: 0.00, epsilon: 0.00\n",
            "episode: 530/1000, step: 307, reward: 0.00, epsilon: 0.00\n",
            "episode: 531/1000, step: 181, reward: 0.00, epsilon: 0.00\n",
            "episode: 532/1000, step: 195, reward: 0.00, epsilon: 0.00\n",
            "episode: 533/1000, step: 103, reward: 0.00, epsilon: 0.00\n",
            "episode: 534/1000, step: 331, reward: 0.00, epsilon: 0.00\n",
            "episode: 535/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 536/1000, step: 85, reward: 0.00, epsilon: 0.00\n",
            "episode: 537/1000, step: 158, reward: 0.00, epsilon: 0.00\n",
            "episode: 538/1000, step: 154, reward: 0.00, epsilon: 0.00\n",
            "episode: 539/1000, step: 138, reward: 0.00, epsilon: 0.00\n",
            "episode: 540/1000, step: 194, reward: 0.00, epsilon: 0.00\n",
            "episode: 541/1000, step: 96, reward: 0.00, epsilon: 0.00\n",
            "episode: 542/1000, step: 59, reward: 0.00, epsilon: 0.00\n",
            "episode: 543/1000, step: 94, reward: 0.00, epsilon: 0.00\n",
            "episode: 544/1000, step: 351, reward: 0.00, epsilon: 0.00\n",
            "episode: 545/1000, step: 302, reward: 0.00, epsilon: 0.00\n",
            "episode: 546/1000, step: 240, reward: 0.00, epsilon: 0.00\n",
            "episode: 547/1000, step: 299, reward: -1.00, epsilon: 0.00\n",
            "episode: 548/1000, step: 60, reward: 0.00, epsilon: 0.00\n",
            "episode: 549/1000, step: 91, reward: 0.00, epsilon: 0.00\n",
            "episode: 550/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 551/1000, step: 170, reward: 0.00, epsilon: 0.00\n",
            "episode: 552/1000, step: 185, reward: 0.00, epsilon: 0.00\n",
            "episode: 553/1000, step: 160, reward: 0.00, epsilon: 0.00\n",
            "episode: 554/1000, step: 54, reward: 0.00, epsilon: 0.00\n",
            "episode: 555/1000, step: 51, reward: 0.00, epsilon: 0.00\n",
            "episode: 556/1000, step: 235, reward: 0.00, epsilon: 0.00\n",
            "episode: 557/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 558/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 559/1000, step: 93, reward: 0.00, epsilon: 0.00\n",
            "episode: 560/1000, step: 162, reward: 0.00, epsilon: 0.00\n",
            "episode: 561/1000, step: 135, reward: 0.00, epsilon: 0.00\n",
            "episode: 562/1000, step: 303, reward: 0.00, epsilon: 0.00\n",
            "episode: 563/1000, step: 239, reward: 0.00, epsilon: 0.00\n",
            "episode: 564/1000, step: 242, reward: 0.00, epsilon: 0.00\n",
            "episode: 565/1000, step: 138, reward: 0.00, epsilon: 0.00\n",
            "episode: 566/1000, step: 395, reward: 0.00, epsilon: 0.00\n",
            "episode: 567/1000, step: 192, reward: 0.00, epsilon: 0.00\n",
            "episode: 568/1000, step: 129, reward: 0.00, epsilon: 0.00\n",
            "episode: 569/1000, step: 52, reward: 0.00, epsilon: 0.00\n",
            "episode: 570/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 571/1000, step: 99, reward: 0.00, epsilon: 0.00\n",
            "episode: 572/1000, step: 159, reward: 0.00, epsilon: 0.00\n",
            "episode: 573/1000, step: 185, reward: 0.00, epsilon: 0.00\n",
            "episode: 574/1000, step: 130, reward: 0.00, epsilon: 0.00\n",
            "episode: 575/1000, step: 56, reward: 0.00, epsilon: 0.00\n",
            "episode: 576/1000, step: 340, reward: 0.00, epsilon: 0.00\n",
            "episode: 577/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 578/1000, step: 188, reward: 0.00, epsilon: 0.00\n",
            "episode: 579/1000, step: 227, reward: 0.00, epsilon: 0.00\n",
            "episode: 580/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 581/1000, step: 292, reward: 0.00, epsilon: 0.00\n",
            "episode: 582/1000, step: 243, reward: 0.00, epsilon: 0.00\n",
            "episode: 583/1000, step: 365, reward: 0.00, epsilon: 0.00\n",
            "episode: 584/1000, step: 59, reward: 0.00, epsilon: 0.00\n",
            "episode: 585/1000, step: 86, reward: 0.00, epsilon: 0.00\n",
            "episode: 586/1000, step: 107, reward: 0.00, epsilon: 0.00\n",
            "episode: 587/1000, step: 133, reward: 0.00, epsilon: 0.00\n",
            "episode: 588/1000, step: 87, reward: 0.00, epsilon: 0.00\n",
            "episode: 589/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 590/1000, step: 128, reward: 0.00, epsilon: 0.00\n",
            "episode: 591/1000, step: 163, reward: 0.00, epsilon: 0.00\n",
            "episode: 592/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 593/1000, step: 76, reward: 0.00, epsilon: 0.00\n",
            "episode: 594/1000, step: 134, reward: 0.00, epsilon: 0.00\n",
            "episode: 595/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 596/1000, step: 219, reward: 0.00, epsilon: 0.00\n",
            "episode: 597/1000, step: 300, reward: 0.00, epsilon: 0.00\n",
            "episode: 598/1000, step: 87, reward: 0.00, epsilon: 0.00\n",
            "episode: 599/1000, step: 238, reward: 0.00, epsilon: 0.00\n",
            "episode: 600/1000, step: 272, reward: 0.00, epsilon: 0.00\n",
            "episode: 601/1000, step: 75, reward: 0.00, epsilon: 0.00\n",
            "episode: 602/1000, step: 75, reward: 0.00, epsilon: 0.00\n",
            "episode: 603/1000, step: 80, reward: 0.00, epsilon: 0.00\n",
            "episode: 604/1000, step: 77, reward: 0.00, epsilon: 0.00\n",
            "episode: 605/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 606/1000, step: 101, reward: 0.00, epsilon: 0.00\n",
            "episode: 607/1000, step: 61, reward: 0.00, epsilon: 0.00\n",
            "episode: 608/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 609/1000, step: 176, reward: 0.00, epsilon: 0.00\n",
            "episode: 610/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 611/1000, step: 204, reward: 0.00, epsilon: 0.00\n",
            "episode: 612/1000, step: 152, reward: 0.00, epsilon: 0.00\n",
            "episode: 613/1000, step: 103, reward: 0.00, epsilon: 0.00\n",
            "episode: 614/1000, step: 179, reward: 0.00, epsilon: 0.00\n",
            "episode: 615/1000, step: 155, reward: 0.00, epsilon: 0.00\n",
            "episode: 616/1000, step: 133, reward: 0.00, epsilon: 0.00\n",
            "episode: 617/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 618/1000, step: 389, reward: 0.00, epsilon: 0.00\n",
            "episode: 619/1000, step: 60, reward: 0.00, epsilon: 0.00\n",
            "episode: 620/1000, step: 157, reward: 0.00, epsilon: 0.00\n",
            "episode: 621/1000, step: 124, reward: 0.00, epsilon: 0.00\n",
            "episode: 622/1000, step: 122, reward: -1.00, epsilon: 0.00\n",
            "episode: 623/1000, step: 191, reward: 0.00, epsilon: 0.00\n",
            "episode: 624/1000, step: 88, reward: 0.00, epsilon: 0.00\n",
            "episode: 625/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 626/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 627/1000, step: 292, reward: 0.00, epsilon: 0.00\n",
            "episode: 628/1000, step: 86, reward: 0.00, epsilon: 0.00\n",
            "episode: 629/1000, step: 160, reward: 0.00, epsilon: 0.00\n",
            "episode: 630/1000, step: 208, reward: 0.00, epsilon: 0.00\n",
            "episode: 631/1000, step: 244, reward: 0.00, epsilon: 0.00\n",
            "episode: 632/1000, step: 179, reward: 0.00, epsilon: 0.00\n",
            "episode: 633/1000, step: 212, reward: 0.00, epsilon: 0.00\n",
            "episode: 634/1000, step: 286, reward: 0.00, epsilon: 0.00\n",
            "episode: 635/1000, step: 173, reward: 0.00, epsilon: 0.00\n",
            "episode: 636/1000, step: 144, reward: 0.00, epsilon: 0.00\n",
            "episode: 637/1000, step: 211, reward: 0.00, epsilon: 0.00\n",
            "episode: 638/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 639/1000, step: 295, reward: 0.00, epsilon: 0.00\n",
            "episode: 640/1000, step: 194, reward: 0.00, epsilon: 0.00\n",
            "episode: 641/1000, step: 111, reward: 0.00, epsilon: 0.00\n",
            "episode: 642/1000, step: 258, reward: 0.00, epsilon: 0.00\n",
            "episode: 643/1000, step: 165, reward: 0.00, epsilon: 0.00\n",
            "episode: 644/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 645/1000, step: 133, reward: 0.00, epsilon: 0.00\n",
            "episode: 646/1000, step: 259, reward: 0.00, epsilon: 0.00\n",
            "episode: 647/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 648/1000, step: 219, reward: 0.00, epsilon: 0.00\n",
            "episode: 649/1000, step: 128, reward: 0.00, epsilon: 0.00\n",
            "episode: 650/1000, step: 174, reward: 0.00, epsilon: 0.00\n",
            "episode: 651/1000, step: 155, reward: 0.00, epsilon: 0.00\n",
            "episode: 652/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 653/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 654/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 655/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 656/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 657/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 658/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 659/1000, step: 400, reward: 0.00, epsilon: 0.00\n",
            "episode: 660/1000, step: 77, reward: 0.00, epsilon: 0.00\n",
            "episode: 661/1000, step: 83, reward: 0.00, epsilon: 0.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5dc46dc90bd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mparam_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mtemp_net_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtemp_net_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mtemp_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_tensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_net_new\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhyper_soft_update\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtemp_net_old\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhyper_soft_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m       \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mstate_dict\u001b[0;34m(self, destination, prefix, keep_vars)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_dict_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mstate_dict\u001b[0;34m(self, destination, prefix, keep_vars)\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0mdestination\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mdestination\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_to_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWKOc49NfUp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}